{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:40:54.056444Z",
     "start_time": "2025-08-29T09:40:51.701750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_model_and_image():\n",
    "    \"\"\"Load a pre-trained ViT model and a sample image.\"\"\"\n",
    "    # Load the model\n",
    "    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    model = ViTForImageClassification.from_pretrained('farleyknight/mnist-digit-classification-2022-09-04')\n",
    "\n",
    "    # Load a sample image\n",
    "    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "    raw_image = Image.open(requests.get(url, stream=True).raw)\n",
    "    image = torch.tensor(np.array(raw_image)).permute(2, 0, 1)\n",
    "\n",
    "    return processor, model, image\n",
    "processor, model, image = load_model_and_image()\n"
   ],
   "id": "a7662bb90c6e6b93",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:44:37.474077Z",
     "start_time": "2025-08-29T09:44:35.512822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "def plot_square(wcopy, **kwargs):\n",
    "    ns = np.ceil(np.sqrt(wcopy.shape[0])).astype(int)\n",
    "    s = np.zeros(ns**2)\n",
    "    s[:wcopy.size] = wcopy\n",
    "    s = s.reshape(ns,ns)\n",
    "    plt.imshow(s, **kwargs)\n",
    "# mms = MinMaxScaler()\n",
    "def get_data(target_digit1=9, target_digit2=1, convert_y=False):\n",
    "    pca = PCA(n_components=4)\n",
    "\n",
    "    mnist = fetch_openml('mnist_784')\n",
    "    X_orig, y_orig = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "    X_orig = np.clip(np.array(X_orig).astype(np.float32) / 255.0, 0.0, 1.0)\n",
    "    y_orig = np.array(y_orig).astype(np.uint8)\n",
    "\n",
    "    target_digit1_xdata = X_orig[y_orig == target_digit1]\n",
    "    target_digit2_xdata = X_orig[y_orig == target_digit2]#[:300]\n",
    "    target_digit1_ydata = y_orig[y_orig == target_digit1]\n",
    "    target_digit2_ydata = y_orig[y_orig == target_digit2]#[:300]\n",
    "    X = np.concatenate((target_digit1_xdata, target_digit2_xdata), axis=0)\n",
    "    y = np.concatenate((target_digit1_ydata, target_digit2_ydata), axis=0)\n",
    "\n",
    "    if convert_y:\n",
    "        y = np.where(y == target_digit1, -1, 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = get_data(3, 8)\n",
    "\n",
    "device = 'cpu'\n",
    "device = 'cpu'\n",
    "# Convert to float for image data and long for classification labels\n",
    "X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "y"
   ],
   "id": "539707fb32e569c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3,  ..., 8, 8, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:44:43.515730Z",
     "start_time": "2025-08-29T09:44:42.646389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2. Reshape, Expand Channels, and Upscale ---\n",
    "\n",
    "# Reshape from (N, 784) to (N, 28, 28)\n",
    "X = X.reshape(-1, 28, 28)\n",
    "\n",
    "# Add a channel dimension: (N, 28, 28) -> (N, 1, 28, 28)\n",
    "X = X.unsqueeze(1)\n",
    "\n",
    "# Repeat the channel dimension 3 times: (N, 1, 28, 28) -> (N, 3, 28, 28)\n",
    "X = X.repeat(1, 3, 1, 1)\n",
    "\n",
    "# Define and apply the upscale transformation\n",
    "resize_transform = torchvision.transforms.Resize((224, 224), antialias=True)\n",
    "X = resize_transform(X)\n",
    "\n",
    "print(f\"Final shape of X: {X.shape}\") # Should be (N, 3, 224, 224)\n",
    "\n",
    "# --- 3. Create Splits using PyTorch Utilities ---\n",
    "\n",
    "# Combine images and labels into a single dataset\n",
    "full_dataset = TensorDataset(X, y)\n",
    "\n",
    "# Define the sizes for your splits (60% train, 20% validation, 20% test)\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.6 * dataset_size)\n",
    "val_size = int(0.2 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size # Ensure all data is used\n",
    "\n",
    "# Perform the split\n",
    "# Use a generator for reproducible splits\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size], generator=generator\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# You are now ready to use these datasets with PyTorch DataLoaders\n",
    "# For example:\n",
    "# from torch.utils.data import DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "465105fdeb03f5dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of X: torch.Size([13966, 3, 224, 224])\n",
      "Train dataset size: 8379\n",
      "Validation dataset size: 2793\n",
      "Test dataset size: 2794\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:44:46.365469Z",
     "start_time": "2025-08-29T09:44:45.384166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=8379)\n",
    "\n",
    "for X, y in train_loader:\n",
    "    print(X.size(), y.size())\n",
    "    break"
   ],
   "id": "56cf1ea36374a1c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8379, 3, 224, 224]) torch.Size([8379])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:44:52.141413Z",
     "start_time": "2025-08-29T09:44:52.071576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = train_dataset[0]\n",
    "logits = model(X.unsqueeze(0)).logits\n",
    "print(y.item(), logits.argmax(-1).item())"
   ],
   "id": "3619c26c1b8f295a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:30:54.107804Z",
     "start_time": "2025-08-29T09:30:53.978723Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5b2bcdabf4de0835",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input image size (28*28) doesn't match model (224*224).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# logits = outputs.logits\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# predicted_class_idx = logits.argmax(-1).item()\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# predicted_class_name = model.config.id2label[predicted_class_idx]\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# return predicted_class_idx, predicted_class_name\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:767\u001B[39m, in \u001B[36mViTForImageClassification.forward\u001B[39m\u001B[34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[39m\n\u001B[32m    759\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    760\u001B[39m \u001B[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[32m    761\u001B[39m \u001B[33;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[32m    762\u001B[39m \u001B[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[32m    763\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m    764\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    765\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m--> \u001B[39m\u001B[32m767\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    768\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    769\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    770\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    771\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    772\u001B[39m \u001B[43m    \u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    773\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    774\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    776\u001B[39m sequence_output = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    778\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.classifier(sequence_output[:, \u001B[32m0\u001B[39m, :])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:556\u001B[39m, in \u001B[36mViTModel.forward\u001B[39m\u001B[34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pixel_values.dtype != expected_dtype:\n\u001B[32m    554\u001B[39m     pixel_values = pixel_values.to(expected_dtype)\n\u001B[32m--> \u001B[39m\u001B[32m556\u001B[39m embedding_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbool_masked_pos\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbool_masked_pos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    560\u001B[39m encoder_outputs = \u001B[38;5;28mself\u001B[39m.encoder(\n\u001B[32m    561\u001B[39m     embedding_output,\n\u001B[32m    562\u001B[39m     head_mask=head_mask,\n\u001B[32m   (...)\u001B[39m\u001B[32m    565\u001B[39m     return_dict=return_dict,\n\u001B[32m    566\u001B[39m )\n\u001B[32m    567\u001B[39m sequence_output = encoder_outputs[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:107\u001B[39m, in \u001B[36mViTEmbeddings.forward\u001B[39m\u001B[34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    101\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    102\u001B[39m     pixel_values: torch.Tensor,\n\u001B[32m    103\u001B[39m     bool_masked_pos: Optional[torch.BoolTensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    104\u001B[39m     interpolate_pos_encoding: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    105\u001B[39m ) -> torch.Tensor:\n\u001B[32m    106\u001B[39m     batch_size, num_channels, height, width = pixel_values.shape\n\u001B[32m--> \u001B[39m\u001B[32m107\u001B[39m     embeddings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpatch_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    109\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m bool_masked_pos \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    110\u001B[39m         seq_length = embeddings.shape[\u001B[32m1\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/tarpoison/.venv/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:162\u001B[39m, in \u001B[36mViTPatchEmbeddings.forward\u001B[39m\u001B[34m(self, pixel_values, interpolate_pos_encoding)\u001B[39m\n\u001B[32m    160\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m interpolate_pos_encoding:\n\u001B[32m    161\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m height != \u001B[38;5;28mself\u001B[39m.image_size[\u001B[32m0\u001B[39m] \u001B[38;5;129;01mor\u001B[39;00m width != \u001B[38;5;28mself\u001B[39m.image_size[\u001B[32m1\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m162\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    163\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInput image size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mheight\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m*\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwidth\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) doesn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt match model\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    164\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.image_size[\u001B[32m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m*\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.image_size[\u001B[32m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m).\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    165\u001B[39m         )\n\u001B[32m    166\u001B[39m embeddings = \u001B[38;5;28mself\u001B[39m.projection(pixel_values).flatten(\u001B[32m2\u001B[39m).transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)\n\u001B[32m    167\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m embeddings\n",
      "\u001B[31mValueError\u001B[39m: Input image size (28*28) doesn't match model (224*224)."
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a51ff8b9972aea15"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
